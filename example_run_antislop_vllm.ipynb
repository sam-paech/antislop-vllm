{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: load the model with vllm:\n",
    "\n",
    "# vllm serve unsloth/gemma-3-1b-it --port 8008 --max-model-len 2500 --served-model-name unsloth/gemma-3-1b-it --gpu-memory-utilization 0.8 --dtype bfloat16 --api-key xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6352a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7dcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Direct prompting mode. Returns a single unslopped output.\n",
    "\n",
    "if True:\n",
    "    cmd = [\n",
    "        'python3', 'main.py',\n",
    "        #'--api-base-url', 'http://localhost:8000/v1',\n",
    "        '--api-key', 'xxx',\n",
    "        '--model-name', 'Qwen/Qwen3-4B',\n",
    "        '--chat-template-model-id', 'Qwen/Qwen3-4B',\n",
    "        '--logging-level', 'INFO',\n",
    "        '--slop-phrases-file', 'banlists/slop_phrases.json',\n",
    "        '--top-n-slop-phrases', '500',\n",
    "        '--regex-blocklist-file', 'banlists/regex_not_x_but_y.json',\n",
    "        '--max-new-tokens', '500',\n",
    "        '--prompt', 'Write a story about a princess. '\n",
    "    ]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Generate an unslopped dataset using prompts from a huggingface sharegpt dataset\n",
    "\n",
    "# Make sure you've launched the vllm openai-compatible server first. You can use something like:\n",
    "\n",
    "# vllm serve unsloth/gemma-3-4b-it --port 8000 --max-model-len 2500 --served-model-name unsloth/gemma-3-4b-it --gpu-memory-utilization 0.95 --dtype bfloat16 --api-key xxx\n",
    "\n",
    "\n",
    "THREADS  = 40\n",
    "MAX_PROMPTS = 40\n",
    "\n",
    "OUT_JSONL     = 'creative_writing_generations.jsonl'\n",
    "HUMAN_PROFILE = 'data/human_writing_profile.json'\n",
    "\n",
    "\n",
    "cmd = [\n",
    "    'python3', 'main.py',\n",
    "    #'--api-base-url', 'http://localhost:8000/v1',\n",
    "    '--api-key', 'xxx',\n",
    "    #'--model-name', 'unsloth/gemma-3-4b-it',\n",
    "    '--model-name', 'Qwen/Qwen3-4B',\n",
    "    '--output-jsonl', OUT_JSONL,\n",
    "    '--input-hf-dataset', 'Nitral-AI/Reddit-SFW-Writing_Prompts_ShareGPT',\n",
    "    #'--input-hf-dataset', 'Nitral-AI/Reddit-NSFW-Writing_Prompts_ShareGPT',\n",
    "    \n",
    "    '--hf-dataset-split', 'train',\n",
    "    '--threads', str(THREADS),\n",
    "    '--max-prompts', str(MAX_PROMPTS),\n",
    "    '--logging-level', 'INFO',\n",
    "    '--slop-phrases-file', 'banlists/slop_phrases.json',\n",
    "    '--top-n-slop-phrases', '500',\n",
    "    '--regex-blocklist-file', 'banlists/regex_not_x_but_y.json',\n",
    "    '--max-new-tokens', '2000',\n",
    "    '--request-mode', 'chunk',  # chunk / stream (stream mode might be buggy)\n",
    "    '--chunk-size', '20', # how many tokens we request from the api before checking it for new violations\n",
    "    '--chat-template-model-id', \"Qwen/Qwen3-4B\",\n",
    "    '--tdpo-pairs-jsonl', 'tdpo-pairs.jsonl'\n",
    "]\n",
    "print('Running:', ' '.join(cmd))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
